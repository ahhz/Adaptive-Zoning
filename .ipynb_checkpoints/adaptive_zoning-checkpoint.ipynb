{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff77a062-496a-4499-a1c6-b9bf901fdaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from random import random\n",
    "from time import time\n",
    "\n",
    "from adaptive_zoning import AdaptiveZoneSystem\n",
    "\n",
    "t = time()\n",
    "\n",
    "# Create a random data set of n points\n",
    "n = 50;\n",
    "origins      = [1 for _ in range(n)]\n",
    "destinations = [1 for _ in range(n)]\n",
    "weight       = [1 for _ in range(n)]\n",
    "points       = [(random(),random()) for _ in range(n)]\n",
    "\n",
    "# Parameters for the sensitivity to distance, and size of neighbourhoods\n",
    "beta = 100\n",
    "nbh_size = 10\n",
    "\n",
    "# Create the zone system\n",
    "zone_system = AdaptiveZoneSystem(origins, destinations, weight, points, beta, nbh_size)\n",
    "\n",
    "print(\"Calculation took: \", time() - t, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afb78bb-36c1-4866-a6f7-5f44a4b9f556",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot samples from the zone system. First select aggregateion to a fixed number of zones.\n",
    "# Second plot a neighbourhood of choice\n",
    "fig, ax = plt.subplots(1, 2, figsize=(6, 3))\n",
    "zone_system.plot_n_clusters_voronoi(10, ax[0])\n",
    "zone_system.plot_neighbourhood_voronoi(3, ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5f3358-2261-4ef4-8a3d-3027a97d00c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely import Point, distance\n",
    "\n",
    "centroid_file = \"../Centroids/MSOA_Dec_2011_PWC_in_England_and_Wales.shp\"\n",
    "commuting_file = \"../Commuting Data/wu03ew_v2.csv\"\n",
    "residence_column = \"Area of residence\"\n",
    "workplace_column = \"Area of workplace\"\n",
    "msoa11_columns = \"msoa11cd\"\n",
    "count_column_original = \"All categories: Method of travel to work\"\n",
    "count_column_target = \"commuters\"\n",
    "\n",
    "centroid_data = gpd.read_file(centroid_file)\n",
    "commuting_data = pd.read_csv(commuting_file)[[residence_column,workplace_column,count_column_original]]\n",
    "\n",
    "commuting_data = commuting_data.rename(columns={count_column_original: count_column_target})\n",
    "# ignore commuting outside of England and Wales\n",
    "commuting_data = commuting_data.loc[commuting_data[workplace_column].isin(centroid_data[msoa11_columns])]\n",
    "\n",
    "centroid_data.set_index(msoa11_columns, inplace=True)\n",
    "\n",
    "commuting_data = pd.merge(commuting_data, centroid_data[['geometry']], left_on=residence_column, right_on=msoa11_columns, how='left')\n",
    "commuting_data = commuting_data.rename(columns={'geometry': 'residence_centroid'})\n",
    "\n",
    "commuting_data = pd.merge(commuting_data, centroid_data[['geometry']], left_on=workplace_column, right_on=msoa11_columns, how='left')\n",
    "commuting_data = commuting_data.rename(columns={'geometry': 'workplace_centroid'})\n",
    "\n",
    "commuting_data['distance'] = commuting_data.apply(lambda row: distance(row['workplace_centroid'], row['residence_centroid']), axis=1)\n",
    "\n",
    "# Calculate weighted distances\n",
    "commuting_data['weighted_distance'] = commuting_data['distance'] * commuting_data['commuters']\n",
    "\n",
    "# Calculate total commuters and total weighted distance\n",
    "total_commuters = commuting_data['commuters'].sum()\n",
    "total_weighted_distance = commuting_data['weighted_distance'].sum()\n",
    "\n",
    "# Calculate average distance\n",
    "average_distance = total_weighted_distance / total_commuters\n",
    "\n",
    "workplace_population = commuting_data[['Area of workplace','commuters']].groupby('Area of workplace').sum()\n",
    "residential_population = commuting_data[['Area of residence','commuters']].groupby('Area of residence').sum()\n",
    "\n",
    "centroid_data = centroid_data.join(workplace_population)\n",
    "centroid_data = centroid_data.rename(columns={\"commuters\": \"workplace_population\"})\n",
    "centroid_data = centroid_data.join(residential_population)\n",
    "centroid_data = centroid_data.rename(columns={\"commuters\": \"residential_population\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a212178-7609-4b28-973a-ed96a0f18c55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from adaptive_zoning import AdaptiveZoneSystem\n",
    "from plot_adaptive_zoning import plot_neighbourhood_voronoi, plot_cluster_voronoi\n",
    "\n",
    "# NOte: in my laptop this takes up to 3 minutes\n",
    "t = time()\n",
    "\n",
    "points = [(pt.x,pt.y) for pt in centroid_data.geometry]\n",
    "leaf_points = points.copy()\n",
    "pop =  centroid_data['residential_population'].to_list()\n",
    "emp =  centroid_data['workplace_population'].to_list() \n",
    "\n",
    "# Using populations as weights, the paper used area\n",
    "weight = pop.copy()\n",
    "\n",
    "beta = 0.11253833770751953 / 1000 # taken from doubly constrained calibration\n",
    "\n",
    "nbh_size = 72 # reduce to 1% of original OD pairs\n",
    "\n",
    "zone_system = AdaptiveZoneSystem(pop, emp, weight, points, beta, nbh_size)\n",
    "\n",
    "print(\"Calculation took: \", time()-t, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639b87e1-5e2d-403f-ab8e-3bd22bb5a4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(18, 9))\n",
    "zone_system.plot_n_clusters_voronoi(1018, ax[0])\n",
    "zone_system.plot_neighbourhood_voronoi(800, ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68461053-9783-4a49-bb28-7d67aaa2ac03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config_helper import get_key\n",
    "openroute_api_key = get_key(\"OpenRouteService\",\"API key\",\"config.ini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18fe6e7-0ffc-4207-bd1e-224728edc2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize results separately, so the nex cell can be run multiple times to collect more data.\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce935ed-e3d1-4a5f-a73c-8ce2bb0f8889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import random\n",
    "\n",
    "from plot_adaptive_zoning import map_to_n_clusters\n",
    "from openroute_distance import bng_to_wgs84\n",
    "from openroute_distance import get_full_distance\n",
    "from openroute_distance import get_halfway_distance\n",
    "\n",
    "def map_to_n_clusters_temp(n, zone_tree, reclass = True):\n",
    "    def get_cluster(index):\n",
    "        p = zone_tree.get_parent(index)\n",
    "        if p != None and p < zone_tree.get_size() - n - 1:\n",
    "            return get_cluster(p)\n",
    "        return p if not reclass else p - (zone_tree.get_size() - n - 1)\n",
    "\n",
    "    return [get_cluster(index) for index in range(zone_tree.get_num_leafs())]\n",
    "    \n",
    "def straightline_distance(a,b):\n",
    "    dx = a[0] - b[0]\n",
    "    dy = a[1] - b[1]\n",
    "    return sqrt(dx**2 + dy**2)\n",
    "    \n",
    "def find_adaptive_neighbour(neighbourhoods, zone_tree, a, b):\n",
    "    while b not in neighbourhoods[a]:\n",
    "        b = zone_tree.get_parent(b)\n",
    "    return b\n",
    "  \n",
    "cluster = map_to_n_clusters_temp(1018,zone_tree, False)\n",
    "\n",
    "points_wgs84 = bng_to_wgs84(points)\n",
    "\n",
    "# instead of sampling any msoa combinations, sample commuters, this will give better distribution of msoa representative of commuting patterns\n",
    "sample_size = 5\n",
    "zones = list(zip(commuting_data['Area of residence'], commuting_data['Area of workplace']))\n",
    "weights = list(commuting_data['commuters'])\n",
    "codes = random.choices(zones, weights, k = sample_size)\n",
    "sample = [(centroid_data.index.get_loc(a),centroid_data.index.get_loc(b)) for a,b in codes]\n",
    "\n",
    "for a,b in sample:\n",
    "    a_trad = cluster[a]\n",
    "    b_trad = cluster[b]\n",
    "\n",
    "    a_adapt = find_adaptive_neighbour(neighbourhoods, zone_tree, b, a)\n",
    "    b_adapt = find_adaptive_neighbour(neighbourhoods, zone_tree, a, b)\n",
    "    \n",
    "    points_wgs84_a = points_wgs84[a]\n",
    "    points_wgs84_b = points_wgs84[b]\n",
    "\n",
    "    points_wgs84_a_trad = points_wgs84[a_trad]\n",
    "    points_wgs84_b_trad = points_wgs84[b_trad]\n",
    "    \n",
    "    points_wgs84_a_adapt = points_wgs84[a_adapt]\n",
    "    points_wgs84_b_adapt = points_wgs84[b_adapt]\n",
    "\n",
    "    points_bgn_a = points[a]\n",
    "    points_bgn_b = points[b]\n",
    "    \n",
    "    points_bgn_a_trad = points[a_trad]\n",
    "    points_bgn_b_trad = points[b_trad]\n",
    "    \n",
    "    points_bgn_a_adapt = points[a_adapt]\n",
    "    points_bgn_b_adapt = points[b_adapt]\n",
    "\n",
    "    straight_full    = straightline_distance(points_bgn_a       ,points_bgn_b)\n",
    "    straight_trad    = straightline_distance(points_bgn_a_trad  ,points_bgn_b_trad)\n",
    "    straight_adapt_1 = straightline_distance(points_bgn_a       ,points_bgn_b_adapt)\n",
    "    straight_adapt_2 = straightline_distance(points_bgn_a_adapt ,points_bgn_b)\n",
    "\n",
    "    network_full     = get_full_distance(   points_wgs84_a       ,points_wgs84_b        ,\"driving-car\", openroute_api_key)['duration']\n",
    "    network_trad     = get_full_distance(   points_wgs84_a_trad  ,points_wgs84_b_trad   ,\"driving-car\", openroute_api_key)['duration']\n",
    "    network_adapt_1  = get_halfway_distance(points_wgs84_a       , points_wgs84_b_adapt ,\"driving-car\",openroute_api_key,True)['duration']\n",
    "    network_adapt_2  = get_halfway_distance(points_wgs84_a_adapt , points_wgs84_b       ,\"driving-car\",openroute_api_key,False)['duration']\n",
    "   \n",
    "    if all([v != None for v in [network_full, network_trad, network_adapt_1, network_adapt_2]]):\n",
    "        \n",
    "        straight_adapt = (straight_adapt_1 + straight_adapt_2)/2\n",
    "        network_adapt = network_adapt_1 + network_adapt_2\n",
    "\n",
    "        # avoid division by zero\n",
    "        if straight_trad == 0 : straight_trad  = 1\n",
    "        if straight_adapt == 0: straight_adapt = 1\n",
    "        \n",
    "        network_trad_best_guess  = straight_full * network_trad / straight_trad\n",
    "        network_adapt_best_guess = straight_full * network_adapt/ straight_adapt\n",
    "        \n",
    "        results.append((network_full, network_trad_best_guess, network_adapt_best_guess))\n",
    "\n",
    "print(len(results))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65988d08-9b63-4169-a2e4-3a97648ae32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def plot_aggregation_comparison(data):\n",
    "    \"\"\"\n",
    "    Plots a scatter plot comparing traditional and adaptive aggregation against accurate values,\n",
    "    when the data is in a list of tuples.\n",
    "\n",
    "    Args:\n",
    "        data: List of tuples, where each tuple contains (accurate_value, traditional_value, adaptive_value).\n",
    "    \"\"\"\n",
    "\n",
    "    accurate_values = [item[0] for item in data]\n",
    "    traditional_values = [item[1] for item in data]\n",
    "    adaptive_values = [item[2] for item in data]\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Scatter plot for traditional aggregation\n",
    "    plt.scatter(accurate_values, traditional_values, label='Traditional Aggregation', marker='o')\n",
    "\n",
    "    # Scatter plot for adaptive aggregation\n",
    "    plt.scatter(accurate_values, adaptive_values, label='Adaptive Aggregation', marker='x')\n",
    "\n",
    "    # Calculate R-squared for traditional aggregation\n",
    "    r2_traditional = r2_score(accurate_values, traditional_values)\n",
    "    plt.text(0.05, 0.95, f'R² (Traditional): {r2_traditional:.3f}', transform=plt.gca().transAxes, verticalalignment='top')\n",
    "\n",
    "    # Calculate R-squared for adaptive aggregation\n",
    "    r2_adaptive = r2_score(accurate_values, adaptive_values)\n",
    "    plt.text(0.05, 0.90, f'R² (Adaptive): {r2_adaptive:.3f}', transform=plt.gca().transAxes, verticalalignment='top')\n",
    "    improvement_factor = (1-r2_traditional)/(1-r2_adaptive)\n",
    "    plt.text(0.05, 0.85, f'Improvement factor: {improvement_factor:.3f}', transform=plt.gca().transAxes, verticalalignment='top')\n",
    "  \n",
    "    # Add the y=x line (accurate prediction line)\n",
    "    min_val = min(min(accurate_values), min(traditional_values), min(adaptive_values))\n",
    "    max_val = max(max(accurate_values), max(traditional_values), max(adaptive_values))\n",
    "\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--', label='y=x (Accurate)')\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Accurate Values')\n",
    "    plt.ylabel('Aggregated Values')\n",
    "    plt.title('Aggregation Comparison - Travel time (s)')\n",
    "    #plt.xlim(0,5400)\n",
    "    #plt.ylim(0,5400)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "plot_aggregation_comparison(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ba6361-e978-417f-a312-dd7f5f78afde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from doubly_constrained_spatial_interaction_model import distance_matrix_from_point\n",
    "from doubly_constrained_spatial_interaction_model import doubly_constrained\n",
    "from doubly_constrained_spatial_interaction_model import calibrate_doubly_constrained\n",
    "\n",
    "points = np.array([(point.x/1000, point.y/1000) for point in centroid_data.geometry])\n",
    "pop =  centroid_data['residential_population'].to_numpy()\n",
    "emp =  centroid_data['workplace_population'].to_numpy() \n",
    "distance_matrix = distance_matrix_from_points(points)\n",
    "beta = calibrate_doubly_constrained(pop,emp, distance_matrix, 14.464)\n",
    "trips, av_distance, a, b = doubly_constrained(pop, emp, distance_matrix, beta,verbose = True)\n",
    "print(av_distance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
