{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0566d138-fe0e-4884-bc8c-ef23e2505fb2",
   "metadata": {},
   "source": [
    "This notebook, accesses the centroid data and commuting data for England and wales at MSOA level for 2011 online. \n",
    "The data is then preprocessed and stored locally in parquet format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0821c9e8-4e0c-4b74-89cb-3c696d95d434",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import json # For parsing JSON responses\n",
    "\n",
    "# Feature server that provides the MSOA centroids for the 2011 Census\n",
    "feature_server_url = \"https://services1.arcgis.com/ESMARspQHYMw9BZ9/arcgis/rest/services/MSOA_Dec_2011_PWC_in_England_and_Wales_2022/FeatureServer/0\"\n",
    "query_url = f\"{feature_server_url}/query\"\n",
    "metadata_url = feature_server_url # URL for server metadata\n",
    "\n",
    "# Get metadata (Total Features and Max Record Count) ---\n",
    "print(\"Fetching server metadata and total feature count...\")\n",
    "try:\n",
    "    # Get server metadata (including maxRecordCount)\n",
    "    meta_params = {'f': 'json'}\n",
    "    meta_response = requests.get(metadata_url, params=meta_params)\n",
    "    meta_response.raise_for_status()\n",
    "    server_metadata = meta_response.json()\n",
    "    # Get the actual max record count from the server if available\n",
    "    max_records_per_request = server_metadata.get('maxRecordCount', 1000)\n",
    "    print(f\"Server's maximum records per request: {max_records_per_request}\")\n",
    "\n",
    "    # Get total feature count that matches the query (1=1 means all)\n",
    "    count_params = {'where': '1=1', 'returnCountOnly': 'true', 'f': 'json'}\n",
    "    count_response = requests.get(query_url, params=count_params)\n",
    "    count_response.raise_for_status()\n",
    "    count_data = count_response.json()\n",
    "    total_features = count_data.get('count')\n",
    "    if total_features is None:\n",
    "        raise ValueError(\"Could not determine total feature count from server.\")\n",
    "    print(f\"Total features to download: {total_features}\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"FATAL ERROR: Could not fetch metadata or feature count: {e}\")\n",
    "    exit() # Stop execution if we can't get vital info\n",
    "except (json.JSONDecodeError, ValueError, KeyError) as e:\n",
    "    print(f\"FATAL ERROR: Could not parse metadata or count response: {e}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"FATAL ERROR during metadata fetch: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Paginate and Fetch Data ---\n",
    "all_features_gdfs = [] # List to hold GeoDataFrames from each request\n",
    "offset = 0\n",
    "\n",
    "print(\"\\nStarting paginated download...\")\n",
    "while offset < total_features:\n",
    "    current_batch_size = min(max_records_per_request, total_features - offset)\n",
    "    print(f\"  Fetching features {offset + 1} to {offset + current_batch_size} (of {total_features})...\")\n",
    "\n",
    "    params = {\n",
    "        'where': '1=1',\n",
    "        'outFields': '*',\n",
    "        'f': 'geojson',\n",
    "        'returnGeometry': 'true',\n",
    "        'resultOffset': offset,\n",
    "        'resultRecordCount': max_records_per_request # Ask for up to the max limit\n",
    "        # Note: some servers might ignore resultRecordCount if it exceeds their internal limit\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(query_url, params=params)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        geojson_text = response.text\n",
    "        if not geojson_text or geojson_text.isspace():\n",
    "            print(f\"Warning: Received empty response text at offset {offset}. Stopping.\")\n",
    "            break\n",
    "\n",
    "        # Check if the response actually contains features before trying to read\n",
    "        try:\n",
    "            geojson_data_check = response.json()\n",
    "            if 'features' not in geojson_data_check or not geojson_data_check['features']:\n",
    "                 print(f\"  -> No features found in response at offset {offset}. Server might have finished early or issue exists.\")\n",
    "                 # It's possible the total count was slightly off, or we've hit the end.\n",
    "                 # Break cleanly assuming we got everything up to this point.\n",
    "                 break\n",
    "        except json.JSONDecodeError:\n",
    "             print(f\"Error: Received non-JSON response at offset {offset}. Content starts with: {geojson_text[:200]}...\")\n",
    "             break # Stop if response is not valid JSON\n",
    "\n",
    "        # Read the GeoJSON chunk directly using GeoPandas\n",
    "        gdf_chunk = gpd.read_file(StringIO(geojson_text))\n",
    "\n",
    "        if gdf_chunk.empty:\n",
    "            print(f\"Warning: GeoDataFrame created from response at offset {offset} is empty, though response wasn't empty. Stopping.\")\n",
    "            break\n",
    "\n",
    "        all_features_gdfs.append(gdf_chunk)\n",
    "        num_returned_in_chunk = len(gdf_chunk)\n",
    "        print(f\"  -> Received {num_returned_in_chunk} features in this batch.\")\n",
    "\n",
    "        # Important: Increment offset by the number actually returned\n",
    "        offset += num_returned_in_chunk\n",
    "\n",
    "        # Safety break if server returns fewer records than requested but we haven't reached the total\n",
    "        # (This might indicate the end of data) or if it somehow returns 0.\n",
    "        if num_returned_in_chunk < max_records_per_request and offset < total_features:\n",
    "             print(\"  -> Received fewer records than requested, assuming end of data.\")\n",
    "             # Sometimes the total count might be slightly off, or this is the last page\n",
    "             # Update total_features to prevent potential infinite loop if count was wrong\n",
    "             total_features = offset\n",
    "             break # Break the loop as we likely got the last page\n",
    "        elif num_returned_in_chunk == 0:\n",
    "             print(f\"  -> Received 0 features at offset {offset}. Stopping pagination.\")\n",
    "             break\n",
    "\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"ERROR downloading data chunk at offset {offset}: {e}\")\n",
    "        print(\"Stopping further downloads due to error.\")\n",
    "        break # Stop on download error\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR processing data chunk at offset {offset}: {e}\")\n",
    "        print(\"Stopping further downloads due to error.\")\n",
    "        break # Stop on processing error (like bad GeoJSON)\n",
    "\n",
    "\n",
    "# --- 3. Combine Results ---\n",
    "centroid_data = gpd.GeoDataFrame() # Initialize an empty GeoDataFrame\n",
    "\n",
    "if all_features_gdfs:\n",
    "    print(\"\\nCombining all downloaded feature batches...\")\n",
    "    try:\n",
    "        # Concatenate all the collected GeoDataFrames\n",
    "        # Ensure CRS is preserved from the first chunk (assuming all chunks have the same CRS)\n",
    "        centroid_data = gpd.GeoDataFrame(\n",
    "            pd.concat(all_features_gdfs, ignore_index=True),\n",
    "            crs=all_features_gdfs[0].crs\n",
    "        )\n",
    "        print(\"All features combined successfully.\")\n",
    "        print(f\"Total features in final GeoDataFrame: {len(centroid_data)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR combining downloaded data: {e}\")\n",
    "        print(\"The resulting 'centroid_data' may be incomplete or empty.\")\n",
    "else:\n",
    "    print(\"\\nNo features were successfully downloaded or combined.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3313eb75-2fe1-4d8c-8bbf-92c40fe657eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "# URL of the zip file containing MSOA level 2011 Commuting Data for England and Wales, in particular the table called \"WU03EW_V2\"\n",
    "zip_file_url = \"https://s3-eu-west-1.amazonaws.com/statistics.digitalresources.jisc.ac.uk/dkan/files/FLOW/wu03ew_v2/wu03ew_v2.zip\"\n",
    "\n",
    "# Specific CSV filename \n",
    "target_csv_filename = \"wu03ew_v2.csv\"\n",
    "\n",
    "\n",
    "try:\n",
    "    # Download the file content using requests\n",
    "    response = requests.get(zip_file_url)\n",
    "    response.raise_for_status()  # Raise an exception for bad status codes (4xx or 5xx)\n",
    " \n",
    "    # Open the zip file from the downloaded content in memory\n",
    "    zip_content_bytes = io.BytesIO(response.content)\n",
    "\n",
    "    # Use zipfile to open the archive and access the specific CSV\n",
    "    with zipfile.ZipFile(zip_content_bytes, 'r') as z:\n",
    "        available_files = z.namelist()\n",
    "        if target_csv_filename in available_files:\n",
    "            # 4. Read the specific CSV file content\n",
    "            with z.open(target_csv_filename) as csv_file:\n",
    "                commuting_data = pd.read_csv(csv_file)\n",
    "        else:\n",
    "            # File not found in archive\n",
    "            raise FileNotFoundError(f\"The specified file '{target_csv_filename}' was not found in the zip archive.\")\n",
    "\n",
    "    print(f\"Successfully loaded '{target_csv_filename}' \")\n",
    "\n",
    "except requests.exceptions.RequestException as req_err:\n",
    "    print(f\"Error during download: {req_err}\")\n",
    "except zipfile.BadZipFile as zip_err:\n",
    "    print(f\"Error: Downloaded file is not a valid zip file or is corrupted. {zip_err}\")\n",
    "except FileNotFoundError as fnf_err:\n",
    "     print(f\"Error: {fnf_err}\") # Specific error for missing file\n",
    "except pd.errors.ParserError as parse_err:\n",
    "     print(f\"Error parsing CSV file: {parse_err}. Check delimiter or file format.\")\n",
    "except UnicodeDecodeError as decode_err:\n",
    "    print(f\"Error decoding CSV file: {decode_err}. Try specifying the encoding, e.g., pd.read_csv(csv_file, encoding='latin1')\")\n",
    "except Exception as e_manual:\n",
    "    print(f\"An unexpected error occurred during : {e_manual}\")\n",
    "\n",
    "commuting_data = commuting_data.rename(columns={\"All categories: Method of travel to work\": \"Commuters\"})\n",
    "\n",
    "# ignore commuting outside of England and Wales\n",
    "commuting_data = commuting_data.loc[commuting_data[\"Area of workplace\"].isin(centroid_data[\"msoa11cd\"])]\n",
    "commuting_data = commuting_data.loc[commuting_data[\"Area of residence\"].isin(centroid_data[\"msoa11cd\"])]\n",
    "\n",
    "# only keep main columns\n",
    "commuting_data = commuting_data[['Area of residence', 'Area of workplace', 'Commuters']]\n",
    "commuting_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4343f7fa-e193-41d8-91c5-b1714ff97c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum workplace and residential population\n",
    "workplace_population = commuting_data[['Area of workplace','Commuters']].groupby('Area of workplace').sum()\n",
    "residential_population = commuting_data[['Area of residence','Commuters']].groupby('Area of residence').sum()\n",
    "\n",
    "centroid_data = pd.merge(centroid_data, workplace_population, left_on=\"msoa11cd\", right_on='Area of workplace',  how='left')\n",
    "centroid_data = centroid_data.rename(columns={\"Commuters\": \"Workplace population\"})\n",
    "centroid_data = pd.merge(centroid_data, residential_population, left_on=\"msoa11cd\",  right_on='Area of residence', how='left')\n",
    "centroid_data = centroid_data.rename(columns={\"Commuters\": \"Residential population\"})\n",
    "centroid_data = centroid_data[['msoa11cd','Workplace population','Residential population','geometry']]\n",
    "centroid_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a05908-19ca-4e06-a1d1-1cc0673c0e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "import geopandas as gpd\n",
    "\n",
    "# Define the data folder name\n",
    "data_folder = 'data'\n",
    "\n",
    "# Create the data folder if it doesn't exist\n",
    "if not os.path.exists(data_folder):\n",
    "    os.makedirs(data_folder)\n",
    "\n",
    "centroid_file = os.path.join(data_folder, 'centroid_data.parquet')\n",
    "commuting_file = os.path.join(data_folder, 'commuting_data.parquet')\n",
    "centroid_data.to_parquet(centroid_file)\n",
    "commuting_data.to_parquet(commuting_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b0895f-54c0-43d9-85fa-55b16ffae098",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
